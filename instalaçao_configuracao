Procedimento após instalação do SO Linux Centos8
Criar usuário com permissão de root
Usuário aluno criado

================================================================
Instalação de itilitarios do SO:

$ sudo yum intall bsip2 unzip rsync wget net-tools

Instalção dos pacotes para futuros acessos verificaçaõ de redes, dowload via linha de comando e deconpactaçao de arquivos

================================================================
Instalando e configurando SSH

É necessário a instalação do Protocolo SSH devido a necessidade de comunicação do CLUSTER. Se tiver 5 maquina sendo uma o namenode e 5 datanode, acomunicação dos servidores é executada via protocolo SSH.
Procedimento para garantir mais segurança. O mesmo é usado para procedimento PseudoDistribuido. Portanto uma maquina tanto para namenode como para datanode.

$ sudo yum install openssh-server openssh-clients

Os dois pacotes serão instalados sendo um para SERVIDOR e outro para CLIENTES. Geralmente o Centos ja vem com os pacotes SSH instalado. Caso não os pacotes serão instalados.

Após a instalção é necessário habilitar o serviço SSH

$ sudo systemctl enable sshd
SSHD - Para rodar o serviço em background

$ sudo systemctl start sshd

Caso queira checar somente usar o status

$ sudo systemctl status sshd
Executando o comando acima o SO irá retornar a informação de:
Active: active(running) destaque

Após procedimento acima é necessário executar configuração no arquivos sshd_config. Para acessar o arquivo segue o comando:

$ sudo gedit /etc/ssh/sshd_config
localizar a porta 22
#Port 22
Necessário tirar o sustenido ou Hashitag
Port 22
OBS: Essa porta 22 é padrão, pois em produção a porta pode ser alterada.

Executar o mesmo procedimento para 
#ListenAddress 0.0.0.0
Necessário tirar o sustenido ou Hashtag

Também localizar no mesmo aquivo 
#PermitRootLogin yes
Tirar o sustenido e alterar de yes para no
PermitRootLogin no

Procedimento para garantir a segurança de conexão do linux
#LoginGraceTime 2m
PermitRootLogin no
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10
AllowUsers aluno hadoop

$ sudo systemctl restart sshd
$ sudo systemctl status sshd
Executando o comando acima o SO irá retornar a informação de:
Active: active(running) destaque


================================================================
Instalando e Configurando Maquina Virtual JAVA JDK
O haddop rodar atraves de uma JVM, portanto sem o Java instalado o Hadoop não irá funcionar, portanto todas as maquina do cluster devem conter o java e o SSH.

$ java -version
Ira retornar a versão do java instalado.

Não usar o OpenJDK - Pois a oracle não atualiza mais esse pacote, pois existe um grupo de pessoas que atualiza, portanto não é recomendado usar o OpenJDK e sim o ORACLEJDK.

ORACLEJDK se tornou padrão.

Então será necessário remover o OpenJDK.
Segue comando:

$ sudo yum list java*
Tudo PAcotes que tiver java sera listado
irá retornar varias versões difrentes.

Para remover segue comando:
$ sudo yum -y remove java*

$ java -version
Após remover não irá aparecer nenhuma versão, pois foram todas removidas

Após o procedimento acima devemos baixar o oraclejdk direto do site da oracle.
pesquisa por java jdk download

Devemos baixar o jdk8 tem todo o ecossitema hadoop essa versão é a mais completa, pois as versões 11 e 12 não esta completa com todos os pacotes necessários para instalção do hadoop.

Não usar Demos and Samples. É necessário baixar o JDK kit

Baixar a versão para Linu 64bit tar_gz
Importa saber que é necessário uma conta no site da oracle para baixar o arquivo
O arquivo será baixado na pasta Download do linux

Siga até a pasta
$ cd Downloads
Estando na pasta Downloads descompactar o arquivo
$ tar -xzf jdk-8u212-linux-x64.tar.gz
aplica ls na pasta para confirmar a descompactação, dever existir um arquivo chamado
jdk1.8.0.212


Para instalação do JDK serpa usado um padrão, portanto vamos mover o arquivo para pasta /opt/
$ sudo mv jdk1.8.0.212/ /opt/jdk

Confirma se o arquivo foi movido
$ cd /opt/
$ ls
volta para o diretorio home $ cd ~

Configura as variaveis de ambientes
home/aluno 
$ ls -la
Ira listar todos os arquivos do diretorio home, isso retornar todos os aquivos ocultos.
$ gedit .bashrc

informar as seguintes variaveis
# JDK
export JAVA_HOME=/opt/jdk
export PATH=$PATH:$JAVA_HOME/bin

Para atualizar as variaveis de ambientes:
$ source .bashrc

confirmar a versão
$ java -version
irá retornar a nova versão instalada (java version "1.8.0_212")

Esse procedimento e extremamente improtante para não causar erros na instalação do hadoop e seu ecossistema.

================================================================

Instalação do Apache Hadoop

Por padrão temos dois usuários Gerados, 

Root que é padrão do SO
Usuário Aluno que foi gerado na instalação do sistema operacional
Será necessário gera outro para instalção do Haddop

Comando id
$ id
Retorna qual usuário esta conectado

Boas Praticas é gerar um usuário expecifio para instalação do Apache Hadoop, importante informa que esse procedimento não se torna obrigatório.

Criando usuário 
$ sudo adduser hadoop
Definindo a senha do usuário gerado
$ sudo passwd hadoop

Após gerar o usuário e difinir a senha é necessário efetuar LOGOUT no sistema para validar (LOGIN) acesso com o usuário hadoop gerado. Processo para garantir padrão de seguraça.

Nesse momento o usuário hadoop é um usuário comum, portanto devemos adicionar previlegios de sudo para usuário hadoop.

Segue comando:
$ su
Acesso com root para atualizar o seguinte arquivo.
# gedit /etc/sudoers

root	ALL=(ALL) 	ALL
aluno	ALL=(ALL)	ALL
hadoop	ALL=(ALL)	ALL 

informar todos os previlegios para os devidos usuários

O hadoop deve ser baixo do site oficial https://hadoop.apache.org

Clique no botão Download >> clicar na ultima versão
Baixe os Bynarios o site ira direcionar para pagina de mirror tar_gz
link HTTP selecionar os links abaixo

Acesse a pasta Downloads
$ cd Downloads
Descompactar aruqivo
$ tar -xvf  hadoop-3.2.0.tar-gz

OBS: Hadoop não tem instalador, para instalar é necessário baixar o arquivo, descompactar.

Após descompactar devemos mover os arquivo para /opt ou qualquer outro diretório de preferencia. Por padrão todas as aplicações serão movidas para o diretorio /opt

Portanto dentro do diretorio Downlods mova para o diretorio de origem.

$ sudo mv hadoop-3.2.0 /opt/hadoop

Após esse procedimento verifique se o diretorio foi movido corretamente dentro do diretorio /opt/hadoop

volte para o diretorio /home
$ cd ~
liste o diretorio home para localizar o arquivo .bashrc
$ ls -la

Esse procedimento irá ser necessário novamente para configurar as variaveis de ambientes
pois devemos informar onde está localizado o diretorio de instalçao do hadoop, que fica dentro do diretorio /opt/hadoop

$ gedit .bashrc

# JDK
export JAVA_HOME=/opt/jdk
export PATH=$PATH:$JAVA_HOME/bin

# Hadoop
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MARED_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

agora devemos validar o arquivo
$ source .bashrc

Verificando 
$ java -version
deverá informar a versão do oracle

$ hadoop version
deve retornar a seguinte informação 
hadoop 3.2.xxxxxx

Agora devemos configurar o hadoop para modo pseudo distribuido como se fosse um cluster de uma maquina só.

Acesse a documentção http://hadoop.apache.org
va em documentação:

Finalidade
Este documento descreve como instalar e configurar uma instalação do Hadoop de nó único, para que você possa executar rapidamente operações simples usando o Hadoop MapReduce e o HDFS (Hadoop Distributed File System).

Pré-requisitos
Plataformas Suportadas
O GNU / Linux é suportado como uma plataforma de desenvolvimento e produção. O Hadoop foi demonstrado em clusters GNU / Linux com 2000 nós.

O Windows também é uma plataforma suportada, mas as etapas a seguir são apenas para Linux. Para configurar o Hadoop no Windows, consulte a página da wiki .

Software Necessário
O software necessário para Linux inclui:

Java ™ deve estar instalado. As versões Java recomendadas são descritas em HadoopJavaVersions .

O ssh deve estar instalado e o sshd deve estar em execução para usar os scripts do Hadoop que gerenciam os daemons remotos do Hadoop se os scripts opcionais de início e parada forem usados. Além disso, recomenda-se que o pdsh também seja instalado para melhor gerenciamento de recursos ssh.

Configuração
Use o seguinte:
Acesso o diretorio /opt/hadoop
$ cd /opt/hadoop
portanto dentro do diretoio de instalçao do hadoop teremos o diretorio etc/

$ cd etc/
dentro do diretorio etc/ teremos o diretorio hadoop
$ cd /etc

segue exemplo:

[hadoop@dataserver /]$ cd /opt/hadoop/
[hadoop@dataserver hadoop]$ ls -la
total 188
drwxr-xr-x. 10 hadoop hadoop    161 nov 26 00:38 .
drwxr-xr-x.  8 root   root      112 dez  1 01:48 ..
drwxr-xr-x.  2 hadoop hadoop    203 set 10 13:51 bin
drwxr-xr-x.  3 hadoop hadoop     20 set 10 12:58 etc
drwxr-xr-x.  2 hadoop hadoop    106 set 10 13:51 include
drwxr-xr-x.  3 hadoop hadoop     20 set 10 13:51 lib
drwxr-xr-x.  4 hadoop hadoop   4096 set 10 13:51 libexec
-rw-rw-r--.  1 hadoop hadoop 150569 set 10 11:35 LICENSE.txt
drwxrwxr-x.  3 hadoop hadoop   4096 dez  1 02:00 logs
-rw-rw-r--.  1 hadoop hadoop  22125 set 10 11:35 NOTICE.txt
-rw-rw-r--.  1 hadoop hadoop   1361 set 10 11:35 README.txt
drwxr-xr-x.  3 hadoop hadoop   4096 set 10 12:58 sbin
drwxr-xr-x.  4 hadoop hadoop     31 set 10 14:11 share
[hadoop@dataserver hadoop]$ cd etc
[hadoop@dataserver etc]$ ls -la
total 4
drwxr-xr-x.  3 hadoop hadoop   20 set 10 12:58 .
drwxr-xr-x. 10 hadoop hadoop  161 nov 26 00:38 ..
drwxr-xr-x.  3 hadoop hadoop 4096 dez  1 00:44 hadoop
[hadoop@dataserver etc]$ 

estando dentro do diretório do hadoop
[hadoop@dataserver etc]$ cd hadoop

segue exemplo:

[hadoop@dataserver hadoop]$ ls
capacity-scheduler.xml            httpfs-log4j.properties     mapred-site.xml
configuration.xsl                 httpfs-signature.secret     shellprofile.d
container-executor.cfg            httpfs-site.xml             ssl-client.xml.example
core-site.xml                     kms-acls.xml                ssl-server.xml.example
hadoop-env.cmd                    kms-env.sh                  user_ec_policies.xml.template
hadoop-env.sh                     kms-log4j.properties        workers
hadoop-metrics2.properties        kms-site.xml                yarn-env.cmd
hadoop-policy.xml                 log4j.properties            yarn-env.sh
hadoop-user-functions.sh.example  mapred-env.cmd              yarnservice-log4j.properties
hdfs-site.xml                     mapred-env.sh               yarn-site.xml
httpfs-env.sh                     mapred-queues.xml.template
[hadoop@dataserver hadoop]$ ^C
[hadoop@dataserver hadoop]$ 

[hadoop@dataserver hadoop]$ pwd
/opt/hadoop/etc/hadoop
[hadoop@dataserver hadoop]$

dentro dediretorio teremo diversos arquivos, portanto vamos configurar o modelo pseudo distribuido.
$ gedit core-site.xml

Inicialmente o arquivo está com os paramentros de configuração vazio.
Exemplo:

<configuration>
    
</configuration>

Devemos preencher comforme segue abaixo:
Para preencher devemos acessar o navegar junto a documantação hadoop.apache.org
e copiar os seguintes parametros, conforme o modelo pseudo distribuido.

Com esses parametros estamos indicando qual é o caminho do HDFS

- sistema de aquivo padrão que é: s.defaultFS
- no caminho: hdfs://localhost:9000 será o ponto de entrada do sistema HDFS, portanto os serviço dserão gerenciados pelo namenode e datanode que são os doi serviços principais.

 <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

Proximo arquivo será :

$ gedit hdfs-site.xml
devemos executar o mesmo procedimento acima, copiando as seguintes parametrizações:

- replicador para cada bloco fs.replication
Portanto caso for executado em maquinas diferente teriamos que informa a quantidade de blocos a serem replicados.
Seve como se fosse um backup, perdendo um bloco teremos mais doi para recuperar caso tenha sido configura 3 blocos.

Em caso de pseudo distribuido iremos usar somente um bloco. no caso da perda do bloco não teremos replica, pois estamos apontando somente um bloco.


<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

========================================================================
Configurando SSH sem a senha
Procedimento usado para configurar a conexão dos Cluster via protocolo SSH.

cahver privada = Namenode

cahve publica = DataNode

Inicie o procedimento dentro do /home/haddop para geração do par de chaves

Comando para gerar a chave (gerado de chave)
$ ssh keygen t rsa
retorna a chave dentro do diretorio (/home/hadoop/ .ssh/id_rsa)
depois o sistema ira perguntar se necessário uma camada de segurança
Enter passphrase (empty for no passphrase) - Não é necessário pressiona enter.
Na sequncia o sistema ira perguntar para confirmar, então pressiona ENTER.

Com esse procedimento a chave será criada.

Para confirmar 
$cd .ssh/


hadoop@dataserver ~]$ cd .ssh/
[hadoop@dataserver .ssh]$ ls -la
total 20
drwx------.  2 hadoop hadoop   80 nov 26 00:15 .
drwx------. 17 hadoop hadoop 4096 dez  6 22:57 ..
-rw-------.  1 hadoop hadoop  399 nov 26 00:10 authorized_keys
-rw-------.  1 hadoop hadoop 1823 nov 26 00:05 id_rsa
-rw-r--r--.  1 hadoop hadoop  399 nov 26 00:05 id_rsa.pub
-rw-r--r--.  1 hadoop hadoop  353 nov 26 00:43 known_hosts
[hadoop@dataserver .ssh]$ 

id_rsa é a chave privada
id_rsa.pub é a chave publica

achave publica dever ser copiada para o arquivo authorized_keys
com o seguinte comando:
volte para o diretorio home
$ cd ~
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

(>>) comando seguinifica gravar

[hadoop@dataserver ~]$ cd .ssh/
[hadoop@dataserver .ssh]$ ls -la
total 20
drwx------.  2 hadoop hadoop   80 nov 26 00:15 .
drwx------. 17 hadoop hadoop 4096 dez  6 22:57 ..
-rw-------.  1 hadoop hadoop  399 nov 26 00:10 authorized_keys
-rw-------.  1 hadoop hadoop 1823 nov 26 00:05 id_rsa
-rw-r--r--.  1 hadoop hadoop  399 nov 26 00:05 id_rsa.pub
-rw-r--r--.  1 hadoop hadoop  353 nov 26 00:43 known_hosts
[hadoop@dataserver .ssh]$ 

Percebemos que o arquivo authorized_keys foi gravado
Abrindo o arquivo
$ gedit authorized_keys
irá retorno o seguuinte arquivo:

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6MWWiuLZDnZ53ibYQIokScWDdrnTcHBRxa3FUve3W8THh2e1ZzFDzmpjDeH+7HsOHacJVvEXL4QSAW9lYlUmUgm2uA0yz4t4cf1xSKvKfvel8bzU1G6owUdHVcCth7uaneWR4Sywv7tao57l0fIphc0RisWy3q2MDYpcKeblD2EO202PhVq/JVG03lVfhXiCOj8wzZn931R3OJNkaWsyzh90PahY+e9ZpAMW8RCrObO7zrK5PWI07qdYMXwEtMcynT+26aZZPMspi+dE9BmMKEMsOwZC/udyImpysT0hLtQ3n7uq97KADIZ3XMD0mYr0GhkHw87k3bhJhpGJhY76P hadoop@dataserver

OBS:
a chave está ciptografada
Como estamo usando um módelo pseudo distribuido tanto a chave piravada como a chave publica fica em um arquivo unico. Caso foi executado para modelo de cluster teriamos todas as chaves conforme seu devido cluster.

Também será necessário mudar a permissão do arquivo.
$ chmod 600 authorized_keys
-rw-------.  1 hadoop hadoop  399 nov 26 00:10 authorized_keys
A partir desse momento somento o dono do arquivo pode ler e escrever dentro desse aquivo.

Volte para o diretorio home 
$ cd ~

Testar a configuração
$ ssh localhost
Sistema irá solictar confirmação para efetivar a conexão ao localhost(127.0.0.1) presscione (yes)

Mesmo assim pediu a senha, stá errado, pois não pode socilitar a senha.

Para configurar a senha é necessário executar o seguinte comando:
$ sudo gedit /etc/ssh/sshd_config
irá pedir a senha 

Caso não abra o aquivo devido a previlegios abra como #ROOT

# sudo gedit /etc/ssh/sshd_config

consequatemente o arquivo será aberto

Na linha AllowUsers incluir o usuário (hadoop), pois configuramos somente o usuário aluno.

#LoginGraceTime 2m
PermitRootLogin no
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10
AllowUsers aluno hadoop

Após executar as configurações no ssh_config, será necessário restart no serviço ssh.

$ sudo systemctl restart sshd
Após esse procedimento o serviço estar inicializado e não deve pedir a senha

Todo o procedimento acimo é referente a usar o ssh, pois não se trata de configuração de hadoop.

execute
$ exit 
Para sair  (logout)

========================================================================

Inicialmente para será necessário formatar o HDFS
$ hdfs namenode -format

sitema irá prepara o sistema de arquivos HDFS, e uma camda adicional sobra o sitema operacional linux

Após executar a formatção, procure pela informação;
Storege directory /tmp/hadoop/dfs/name has been sucessfully formatted:

A partir desse momento devemos iniciar o HDFS
$ start-dfs.sh

Em caso de problema podemos para todos os serviços o somente o namenode 
$ stop-all.sh

$ hadoop-daemon.sh

[root@dataserver hadoop]# su hadoop
[hadoop@dataserver ~]$ pwd
/home/hadoop
[hadoop@dataserver ~]$ start-dfs.sh
Starting namenodes on [localhost]
localhost: /opt/hadoop/libexec/hadoop-functions.sh: linha 1848: /tmp/hadoop-hadoop-namenode.pid: Permissão negada
localhost: ERROR:  Cannot write namenode pid /tmp/hadoop-hadoop-namenode.pid.
Starting datanodes
Starting secondary namenodes [dataserver]

$ jps
para visualizar os serviços que estão rodando.

[hadoop@dataserver ~]$ jps
5952 SecondaryNameNode
6225 Jps
5622 NameNode
5750 DataNode


acesse via browser;
http://localhost:9870
Possibilta gerenciar os namenode e datanode via browser

aogra vamos criar alguma coisa no hadoop
$ hdfs dfs -ls /
como não temos nada no diretório raiz do hadoop não irá listar nada.

portanto vamos criar um diretório
$ hdfs dfs -mkdir /user
$ hdfs dfs -mkdir /user/hadoop

copiando conteudo para pasta gerada
$ hdfs dfs -put /opt/hadoop/etc/hadoop/*.xml /user/hadoop
lembrando que não está movendo e sim copiando

$ hdfs dfs -ls /user/hadoop

Found 13 items
-rw-r--r--   1 hadoop supergroup       8260 2019-11-26 22:54 /user/hadoop/capacity-scheduler.xml
-rw-r--r--   1 hadoop supergroup        885 2019-11-26 22:54 /user/hadoop/core-site.xml
-rw-r--r--   1 hadoop supergroup      11392 2019-11-26 22:54 /user/hadoop/hadoop-policy.xml
-rw-r--r--   1 hadoop supergroup        868 2019-11-26 22:54 /user/hadoop/hdfs-site.xml
-rw-r--r--   1 hadoop supergroup        620 2019-11-26 22:54 /user/hadoop/httpfs-site.xml
drwxr-xr-x   - hadoop supergroup          0 2019-12-01 00:12 /user/hadoop/input
-rw-r--r--   1 hadoop supergroup       3518 2019-11-26 22:54 /user/hadoop/kms-acls.xml
-rw-r--r--   1 hadoop supergroup        682 2019-11-26 22:54 /user/hadoop/kms-site.xml
-rw-r--r--   1 hadoop supergroup        758 2019-11-26 22:54 /user/hadoop/mapred-site.xml
drwxr-xr-x   - hadoop supergroup          0 2019-11-27 23:57 /user/hadoop/output
drwxr-xr-x   - hadoop supergroup          0 2019-12-01 00:01 /user/hadoop/output2
drwxr-xr-x   - hadoop supergroup          0 2019-12-01 00:17 /user/hadoop/output3
-rw-r--r--   1 hadoop supergroup        690 2019-11-26 22:54 /user/hadoop/yarn-site.xml


========================================================================

Processando Big Data com Hadoop

Nesse procedimento será usado MapReduce

Aplicando MapReduce
Iremos usar um job mapreduce feito em java

Localizando os Jobs:
$ cd /opt/hadoop/share/hadoop/
$ ls -la

total 12
drwxr-xr-x. 8 hadoop hadoop   88 set 10 13:51 .
drwxr-xr-x. 4 hadoop hadoop   31 set 10 14:11 ..
drwxr-xr-x. 2 hadoop hadoop  123 set 10 13:51 client
drwxr-xr-x. 6 hadoop hadoop  184 set 10 12:58 common
drwxr-xr-x. 6 hadoop hadoop 4096 set 10 13:04 hdfs
drwxr-xr-x. 6 hadoop hadoop 4096 set 10 13:51 mapreduce
drwxr-xr-x. 6 hadoop hadoop   68 set 10 13:51 tools
drwxr-xr-x. 8 hadoop hadoop 4096 set 10 13:51 yarn

$ cd ls -la
drwxr-xr-x. 6 hadoop hadoop    4096 set 10 13:51 .
drwxr-xr-x. 8 hadoop hadoop      88 set 10 13:51 ..
-rw-r--r--. 1 hadoop hadoop  613301 set 10 13:36 hadoop-mapreduce-client-app-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop  805845 set 10 13:36 hadoop-mapreduce-client-common-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop 1657002 set 10 13:36 hadoop-mapreduce-client-core-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop  215919 set 10 13:36 hadoop-mapreduce-client-hs-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop   45619 set 10 13:36 hadoop-mapreduce-client-hs-plugins-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop   85900 set 10 13:36 hadoop-mapreduce-client-jobclient-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop 1660369 set 10 13:36 hadoop-mapreduce-client-jobclient-3.2.1-tests.jar
-rw-r--r--. 1 hadoop hadoop  126430 set 10 13:36 hadoop-mapreduce-client-nativetask-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop   97738 set 10 13:36 hadoop-mapreduce-client-shuffle-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop   57934 set 10 13:36 hadoop-mapreduce-client-uploader-3.2.1.jar
-rw-r--r--. 1 hadoop hadoop  316534 set 10 13:36 hadoop-mapreduce-examples-3.2.1.jar
drwxr-xr-x. 2 hadoop hadoop    4096 set 10 13:51 jdiff
drwxr-xr-x. 2 hadoop hadoop      57 set 10 13:51 lib
drwxr-xr-x. 2 hadoop hadoop      30 set 10 13:51 lib-examples
drwxr-xr-x. 2 hadoop hadoop    4096 set 10 13:51 sources

Esses arquivos listado acima são jobs pre-desenvolvidos de modelos MapReduce

Com o procedimento de job MarReduce termos um grande volume de dados e extrair um conjunto de informações reduzidas.

segue comando:

$ hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep /user/hadoop output 'dfs[a-z.]+'

Trauzindo o comando;
1 - hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep /user/hadoop output 'dfs[a-z.]+'

2 - grep /user/hadoop output 'dfs[a-z]+'
    pegar os arquivos dentro do diretorio e crie um diretorio (output) e tenha palavras que comece com dfs e que tenha a e z

 Depois execute o camando será gerado um job

 primeiro ele irá mapear 100% e depois irá reduzir 100%

 Verificando o resultado do processamento
 $ hdfs dfs -ls /user/hadoop/output

 usei 3 repositorios (diretorio) alem do output

 Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2019-11-27 23:57 /user/hadoop/output/_SUCCESS
-rw-r--r--   1 hadoop supergroup         11 2019-11-27 23:57 /user/hadoop/output/part-r-00000
[hadoop@dataserver ~]$ hdfs dfs -ls /user/hadoop/output2
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2019-12-01 00:01 /user/hadoop/output2/_SUCCESS
-rw-r--r--   1 hadoop supergroup          0 2019-12-01 00:01 /user/hadoop/output2/part-r-00000
[hadoop@dataserver ~]$ hdfs dfs -ls /user/hadoop/output3
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2019-12-01 00:17 /user/hadoop/output3/_SUCCESS
-rw-r--r--   1 hadoop supergroup         29 2019-12-01 00:17 /user/hadoop/output3/part-r-00000
[hadoop@dataserver ~]$ 

o primeiro arquivo representa 0 bytes com sucesso
o segundo arquivo representa 11 bytes pois com o nome padrão do hadoop (part-r-00000)

OBS: O hadoop não foi feito para executar ou visualizar os arquivos gerados, portanto devemos copiar o arquivo gerado para maquina local possibilitando a visualização dos dados gerados atraves do MapReduce.

Como fazer isso:
$ hdfs dfs -get output /output

OBS: procedimento de copiar o aquivos pode ser executado para qualquer diretorio para melhor controle.

$ ls
'Área de trabalho'   Documentos   Downloads   Imagens   Modelos   Música   output   Público   Vídeos

[hadoop@dataserver ~]$ cd output/
[hadoop@dataserver output]$ ls
part-r-00000  _SUCCESS
[hadoop@dataserver output]$ 


[hadoop@dataserver output]$ gedit _SUCCESS 
[hadoop@dataserver output]$ gedit part-r-00000 

Após concluir todo o trabalho saia da aplicação com o seguinte comando:

========================================================================
stop-dfs.sh
========================================================================
Para iniciar é:
start-dfs.sh
========================================================================




========================================================================
Configurando YARN
========================================================================

O YARN é um software para gerenciar jobs processar,monitorar e gerenciar, pois podemos considerar o YARN como o terceiro software do ecosistema. 

HDFS - Para armazenamento distribuido
MAPREDUCE - Para processamento distribuido
YARN - Para gerenciamento distribuido

Primeiramente será necessário configurar os parametros de dois arquivos.

1 - mapred-site.xml

2 - yarn-site.xml

Acesse primeiro o mapred-site.xml com o seguinte comando.
$ gedit /opt/hadoop/etc/hadoop/mapred-site.xml 

Copie os parametros conforme documentação.

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

Deposi o segundo yarn-site.xml como o seguinte arquivo
$ gedit /opt/hadoop/etc/hadoop/yarn-site.xml

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>                    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

=============================================================================================

ficar atento no arquivo yarn-site.xml, existe erros no intervalo dos parametros do arquivo original. Mantenha o arquivo exatamente da forma como est'abaixo:


<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->


<!-- Site specific YARN configuration properties -->

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>                    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
=============================================================================================


Depois de parametrizar todos os devidos parametros, será necessário inicializar os serviços

execute:
================================================
$ start-yarn.sh 
================================================

Ante de iniciar o serviço tinhamos a seguinte liste, depois de iniciar o yarn vamos ter:
$ jps
4930 Jps
4515 DataNode
4727 SecondaryNameNode
4383 NameNode

Depois de iniciar o yarn:
5600 Jps
4515 DataNode
4727 SecondaryNameNode
5226 NodeManager
5102 ResourceManager
4383 NameNode

Nota-se que após iniciar o yarn já temos os seguintes serviços;
Starting resourcemanager
Starting nodemanagers

Caso apresente erros, será necessário verificar nos logs. Como acessar os logs:
$ cd /opt/hadoop/logs

segue os arquivos de logs;

[hadoop@dataserver ~]$ cd /opt/hadoop/logs/
[hadoop@dataserver logs]$ ls -la
total 1648
drwxrwxr-x.  3 hadoop hadoop   4096 dez  7 18:33 .
drwxr-xr-x. 10 hadoop hadoop    161 nov 26 00:38 ..
-rw-rw-r--.  1 hadoop hadoop 299105 dez  7 18:37 hadoop-hadoop-datanode-dataserver.log
-rw-rw-r--.  1 hadoop hadoop    703 dez  7 18:30 hadoop-hadoop-datanode-dataserver.out
-rw-rw-r--.  1 hadoop hadoop    703 dez  7 00:29 hadoop-hadoop-datanode-dataserver.out.1
-rw-rw-r--.  1 hadoop hadoop    703 dez  1 00:45 hadoop-hadoop-datanode-dataserver.out.2
-rw-rw-r--.  1 hadoop hadoop    703 nov 28 00:35 hadoop-hadoop-datanode-dataserver.out.3
-rw-rw-r--.  1 hadoop hadoop    703 nov 26 23:31 hadoop-hadoop-datanode-dataserver.out.4
-rw-rw-r--.  1 hadoop hadoop    703 nov 26 01:04 hadoop-hadoop-datanode-dataserver.out.5
-rw-rw-r--.  1 hadoop hadoop 427750 dez  7 18:32 hadoop-hadoop-namenode-dataserver.log
-rw-rw-r--.  1 hadoop hadoop    874 dez  7 18:30 hadoop-hadoop-namenode-dataserver.out
-rw-rw-r--.  1 hadoop hadoop   6427 dez  7 00:37 hadoop-hadoop-namenode-dataserver.out.1
-rw-rw-r--.  1 hadoop hadoop    874 dez  1 02:00 hadoop-hadoop-namenode-dataserver.out.2
-rw-rw-r--.  1 hadoop hadoop    874 dez  1 00:45 hadoop-hadoop-namenode-dataserver.out.3
-rw-rw-r--.  1 hadoop hadoop    874 nov 28 00:35 hadoop-hadoop-namenode-dataserver.out.4
-rw-rw-r--.  1 hadoop hadoop   6427 nov 26 23:45 hadoop-hadoop-namenode-dataserver.out.5
-rw-rw-r--.  1 hadoop hadoop 331703 dez  7 18:33 hadoop-hadoop-nodemanager-dataserver.log
-rw-rw-r--.  1 hadoop hadoop   2275 dez  7 18:33 hadoop-hadoop-nodemanager-dataserver.out
-rw-rw-r--.  1 hadoop hadoop   2282 dez  1 00:46 hadoop-hadoop-nodemanager-dataserver.out.1
-rw-rw-r--.  1 hadoop hadoop 215013 dez  7 18:33 hadoop-hadoop-resourcemanager-dataserver.log
-rw-rw-r--.  1 hadoop hadoop   2291 dez  7 18:33 hadoop-hadoop-resourcemanager-dataserver.out
-rw-rw-r--.  1 hadoop hadoop   2298 dez  1 00:46 hadoop-hadoop-resourcemanager-dataserver.out.1
-rw-rw-r--.  1 hadoop hadoop 276000 dez  7 18:32 hadoop-hadoop-secondarynamenode-dataserver.log
-rw-rw-r--.  1 hadoop hadoop    703 dez  7 18:30 hadoop-hadoop-secondarynamenode-dataserver.out
-rw-rw-r--.  1 hadoop hadoop    703 dez  7 00:29 hadoop-hadoop-secondarynamenode-dataserver.out.1
-rw-rw-r--.  1 hadoop hadoop    703 dez  1 00:45 hadoop-hadoop-secondarynamenode-dataserver.out.2
-rw-rw-r--.  1 hadoop hadoop    703 nov 28 00:35 hadoop-hadoop-secondarynamenode-dataserver.out.3
-rw-rw-r--.  1 hadoop hadoop    703 nov 26 23:31 hadoop-hadoop-secondarynamenode-dataserver.out.4
-rw-rw-r--.  1 hadoop hadoop    703 nov 26 01:04 hadoop-hadoop-secondarynamenode-dataserver.out.5
-rw-rw-r--.  1 hadoop hadoop      0 nov 26 00:38 SecurityAuth-hadoop.audit
-rw-r--r--.  1 root   root        0 nov 26 00:53 SecurityAuth-root.audit
drwxr-xr-x.  2 hadoop hadoop      6 dez  1 04:17 userlogs

Executado os passos acima podemos visitar o seguinte endereço:
localhost:8088

iremos acessar o gerenciador YARN

Agora vamos executar o mesmo job que foi executado anteriormente. Para isso devemos alterar o diretorio que será salvos os aquivos do MapReduce.

Portanto não será mais o output e sim o output4

hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep /user/hadoop output4 'dfs[a-z.]+'

Esse processo pode gerar erro conforme o short do gerenciador YARN.
Então vamos gera um diretorio somente com os xml's.

Devemos copiar todos os xml's para um novo diretorio:
$ hdfs dfs -put /opt/hadoop/etc/hadoop/*.xml input

Esse comando iré copiar todos os xml's para o diretorio input, portanto agora teremos os daddos que necessitamos para rodar o mapreduce, pois existe somente arquivos e não diretorios na pasta de leitura do mapreduce.

=======================================================================


=======================================================================

Instalando e configurando o Zookeeper

Inicialmente devemos acessar o site https://hadoopecosystemtable.github.io/

Localize o Zookeeper, devemos ser direcionados para o Apche Zookeeper.

Teremos dois links ao lado

1 - Apache Zookeeper
http://zookeeper.apache.org/ >> 
Download ZooKeeper from the release page. >>  
Download ---- https://www.apache.org/dyn/closer.cgi/zookeeper/ >>
HTTP ----- http://ftp.unicamp.br/pub/apache/zookeeper/
zookeeper-3.5.6/

Baixe os Binarios
apache-zookeeper-3.5.6-bin.tar.gz 

Dica --- Copie o link va para o terminal e execute os seguintes comandos
$ cd Downloads
$ wget http://ftp.unicamp.br/pub/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz

Com esse processo o arquivo será baixado.

Os arquivos serão baixados no diretorio Downloads
[hadoop@dataserver ~]$ cd Downloads/
[hadoop@dataserver Downloads]$ ls -la
total 586552
drwxr-xr-x.  2 hadoop hadoop       104 dez  1 10:42 .
drwx------. 17 hadoop hadoop      4096 dez  9 23:13 ..
-rw-rw-r--.  1 hadoop hadoop   9230052 out 15 21:35 apache-zookeeper-3.5.6-bin.tar.gz
-rw-rw-r--.  1 hadoop hadoop 359196911 nov 25 23:07 hadoop-3.2.1.tar.gz
-rw-rw-r--.  1 hadoop hadoop 232190985 out 25 01:08 hbase-2.2.2-bin.tar.gz
[hadoop@dataserver Downloads]$ 

Descompactar o arquivo:
tar -xvf apache-zookeeper-3.5.6-bin.tar.gz

Irá gerar um diretorio, devemos mover esse diretorio para pasta /opt

$ sudo mv apache-zookepeer-3.5.5-bin /opt/zookeeper
acesse o diretorio /opt/zookeeper
$ cd /opt/zookeeper

dentro do diretorio zookeeper crie um diretorio data
$ mkdir data
drwxrwxr-x. 8 hadoop hadoop   158 dez  1 02:02 .
drwxr-xr-x. 8 root   root     112 dez  1 01:48 ..
drwxr-xr-x. 2 hadoop hadoop   232 out  8 17:14 bin
drwxr-xr-x. 2 hadoop hadoop    92 dez  1 01:54 conf
drwxrwxr-x. 3 hadoop hadoop    51 dez  1 02:02 data
drwxr-xr-x. 5 hadoop hadoop  4096 out  8 17:15 docs
drwxrwxr-x. 2 hadoop hadoop  4096 dez  1 01:45 lib
-rw-r--r--. 1 hadoop hadoop 11358 out  5 08:27 LICENSE.txt
drwxrwxr-x. 2 hadoop hadoop    52 dez  1 02:02 logs
-rw-r--r--. 1 hadoop hadoop   432 out  8 17:14 NOTICE.txt
-rw-r--r--. 1 hadoop hadoop  1560 out  8 17:14 README.md
-rw-r--r--. 1 hadoop hadoop  1347 out  5 08:27 README_packaging.txt



acesse o diretorio conf
$ cd conf
teremos um arquivo chamado zoo_sample.cfg de exemplo, portanto devemos copiar esse arquivo e trocar o nome para zoo.cfg, somente crie uma copia

$ cp zoo_sample.cfg zoo.cfg
wxr-xr-x. 2 hadoop hadoop   92 dez  1 01:54 .
drwxrwxr-x. 8 hadoop hadoop  158 dez  1 02:02 ..
-rw-r--r--. 1 hadoop hadoop  535 out  5 08:27 configuration.xsl
-rw-r--r--. 1 hadoop hadoop 2712 out  5 08:27 log4j.properties
-rw-r--r--. 1 hadoop hadoop  931 dez  1 01:54 zoo.cfg
-rw-r--r--. 1 hadoop hadoop  922 out  8 17:14 zoo_sample.cfg

Abra o arquivo zoo.cfg
$ gedit zoo.cfg

# The number of milliseconds of each tick
tickTime=2000

# The number of ticks that the initial 
# synchronization phase can take
initLimit=5

# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5

# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/opt/zookeeper/data

# the port at which the clients will connect
clientPort=2181

# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

Esses é o arquivo de configuração do zookepeer

Após executar o procedimento acima será necessário definir as variaveis de ambiente.

Devemos ir para o diretorio home
$ gedit .bashrc
# Zookeeper
export ZOOKEEPER_HOME=/opt/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

Após executar a definição dos parametros devemos atualizar as configurações atraves do comando:
$ source .bashrc

OBS: não esquecer de inicializar o hadoop e Yarn

Para inicializar o Zookeeper segue o comando:
=======================================================================
$ zkServer.sh start
=======================================================================
[root@dataserver hadoop]# zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@dataserver hadoop]# ^C
[root@dataserver hadoop]# 

Após iniciar devemos executar 
$ jps


=======================================================================


=======================================================================

Instalando e configurando o Hbase

Bando de dados não relacional

Inicialmente devemos acessar o site https://hadoopecosystemtable.github.io/

Localize o HBASE, ao lado
1. Apache HBase Home >> https://hbase.apache.org/

Clique em Download

Download

Click here to download Apache HBase™.
Acesse a versão mais atualizada no link (bin) >> https://www.apache.org/dyn/closer.lua/hbase/2.2.2/hbase-2.2.2-bin.tar.gz

HTTP

http://ftp.unicamp.br/pub/apache/hbase/2.2.2/hbase-2.2.2-bin.tar.gz

Clique com o direito copia o link e acesso o terminal.
acesse o diretorio Downloads

$ cd Downloads
$ wget http://ftp.unicamp.br/pub/apache/hbase/2.2.2/hbase-2.2.2-bin.tar.gz
Download será executado

Após o termino do Download, devemos descompactar o arquivo:
$ tar -xvf hbase-2.2.2-bin.tar.gz

consequentemente movemos o arquivo gerado:
$ sudo mv hbase-2.2.2 /opt/hbase

Acesse o diretorio /opt
$ cd /opt/hbase
crie um diretorio chamado hfiles -- esse diretorio ira ficar os arquivos do banco de dados HBASE
$ mkdir hfiles
total 896
drwxrwxr-x.  8 hadoop hadoop    196 dez 10 00:51 .
drwxr-xr-x.  9 root   root      125 dez 10 00:46 ..
drwxr-xr-x.  4 hadoop hadoop   4096 out 19 06:53 bin
-rw-r--r--.  1 hadoop hadoop 139417 out 19 06:53 CHANGES.md
drwxr-xr-x.  2 hadoop hadoop    208 out 19 06:53 conf
drwxr-xr-x. 11 hadoop hadoop   4096 out 19 07:38 docs
drwxr-xr-x.  7 hadoop hadoop     80 out 19 07:13 hbase-webapps
drwxrwxr-x.  2 hadoop hadoop      6 dez 10 00:51 hfiles
-rw-r--r--.  1 hadoop hadoop    262 out 19 06:53 LEGAL
drwxrwxr-x.  6 hadoop hadoop   8192 dez 10 00:45 lib
-rw-r--r--.  1 hadoop hadoop 129312 out 19 07:44 LICENSE.txt
-rw-r--r--.  1 hadoop hadoop 520601 out 19 07:44 NOTICE.txt
-rw-r--r--.  1 hadoop hadoop   1477 out 19 06:53 README.txt
-rw-r--r--.  1 hadoop hadoop  88280 out 19 06:53 RELEASENOTES.md


acesse o arquivos conf -- Arquivos de configuração para o ecossitema hadoop
$ cd conf
$ ls -la
total 44
drwxr-xr-x. 2 hadoop hadoop  208 out 19 06:53 .
drwxrwxr-x. 8 hadoop hadoop  196 dez 10 00:51 ..
-rw-r--r--. 1 hadoop hadoop 1811 out 19 06:53 hadoop-metrics2-hbase.properties
-rw-r--r--. 1 hadoop hadoop 4284 out 19 06:53 hbase-env.cmd
-rw-r--r--. 1 hadoop hadoop 7536 out 19 06:53 hbase-env.sh
-rw-r--r--. 1 hadoop hadoop 2257 out 19 06:53 hbase-policy.xml
-rw-r--r--. 1 hadoop hadoop  934 out 19 06:53 hbase-site.xml
-rw-r--r--. 1 hadoop hadoop 1168 out 19 06:53 log4j-hbtop.properties
-rw-r--r--. 1 hadoop hadoop 4977 out 19 06:53 log4j.properties
-rw-r--r--. 1 hadoop hadoop   10 out 19 06:53 regionservers

devemos editar o seguinte arquivo:
$ gedit hbase-env.sh

descomente o export e ajuste o caminho da pasta jdk, conforme configurado lá no inicio do documento.

# The java implementation to use.  Java 1.8+ required.
export JAVA_HOME=/opt/jdk/


Devemos editar também o arquivo:
$ gedit hbase-site.xml

<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>file:///opt/hbase/hfiles</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>      
        <value>/opt/zookeeper/data</value>
    </property>
</configuration>

Executado o procedimento acima volte para o diretorio home
$ cd ~
Devemos configurar as variaveis de ambiente
$ gedit .bashrc

# HBase
export HBASE_HOME=/opt/hbase
export PATH=$PATH:$HBASE_HOME/bin

após executar todo o processo acima devemos inicializar o HBASE com o seguinte caminho:

==============================================================
$ start-hbase.sh
==============================================================

Após inicializar o hbase devemos acessar o SHELL com o seguinte comando:

$ hbase shell

Caso fique muito lento podemos parar alguns serviços.

Para sair somente aplicar 
$ exit


=======================================================================



Instalando e configurando o Hive

Apache Hive:
Infraestrutura de Data Warehouse desenvolvida pelo Facebook. Resumo, consulta e análise de dados. Ele fornece uma linguagem semelhante a SQL (não compatível com SQL92): HiveQL.
Portanto possibilita trabalhar dentro do HDFS de forma estruturada.

Inicialmente devemos acessar o site https://hadoopecosystemtable.github.io/

Localize o Apache Hive, ao lado
1. Apache Hive >> http://hive.apache.org/

Clique em Download
Pontos de atenção, devemos verificar qual versão do hive é compativel com o hadoop instalado, portanto estamos trabalhando com a versão 3.1 do hadoop consequentemente devemos usar a versão do hive release 3.1 

Clique em >> Download a release now!

será direcionado para a seguinte pagina >>  http://www.apache.org/dyn/closer.cgi/hive/

clique em >>
HTTP

http://ftp.unicamp.br/pub/apache/hive/

clique em >>
hive-3.1.2/

Usar o binário
apache-hive-3.1.2-bin.tar.gz

Após o termino do Download, devemos descompactar o arquivo:
$ tar -xvf apache-hive-3.1.2-bin.tar.gz

consequentemente movemos o arquivo gerado:
$ sudo mv apache-hive-3.1.2 /opt/hive

dentro do diretório /opt/hive
[hadoop@dataserver hive]$ ls -la
total 56
drwxrwxr-x. 10 hadoop hadoop   184 jan 13 11:38 .
drwxr-xr-x. 10 root   root     137 jan 13 11:39 ..
drwxrwxr-x.  3 hadoop hadoop   157 jan 13 11:38 bin
drwxrwxr-x.  2 hadoop hadoop  4096 jan 13 11:38 binary-package-licenses
drwxrwxr-x.  2 hadoop hadoop  4096 jan 13 11:38 conf
drwxrwxr-x.  4 hadoop hadoop    34 jan 13 11:38 examples
drwxrwxr-x.  7 hadoop hadoop    68 jan 13 11:38 hcatalog
drwxrwxr-x.  2 hadoop hadoop    44 jan 13 11:38 jdbc
drwxrwxr-x.  4 hadoop hadoop 12288 jan 13 11:38 lib
-rw-r--r--.  1 hadoop hadoop 20798 ago 22 18:45 LICENSE
-rw-r--r--.  1 hadoop hadoop   230 ago 22 18:45 NOTICE
-rw-r--r--.  1 hadoop hadoop  2469 ago 22 18:47 RELEASE_NOTES.txt
drwxrwxr-x.  4 hadoop hadoop    35 jan 13 11:38 scripts

acesse o diretório /conf
[hadoop@dataserver hive]$ cd conf/
[hadoop@dataserver conf]$ ls -la
total 336
drwxrwxr-x.  2 hadoop hadoop   4096 jan 13 11:38 .
drwxrwxr-x. 10 hadoop hadoop    184 jan 13 11:38 ..
-rw-r--r--.  1 hadoop hadoop   1596 ago 22 18:44 beeline-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop 300482 ago 22 19:01 hive-default.xml.template
-rw-r--r--.  1 hadoop hadoop   2365 ago 22 18:44 hive-env.sh.template
-rw-r--r--.  1 hadoop hadoop   2274 ago 22 18:45 hive-exec-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   3086 ago 22 18:44 hive-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   2060 ago 22 18:44 ivysettings.xml
-rw-r--r--.  1 hadoop hadoop   3558 ago 22 18:44 llap-cli-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   7163 ago 22 18:44 llap-daemon-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   2662 ago 22 18:44 parquet-logging.properties

acessado o diretorio conf
teremos um arquivo chamado hive-env.sh.template de exemplo, portanto devemos copiar esse arquivo e trocar o nome para hive-env.sh, somente crie uma copia

$ cp hive-env.sh.template hive-env.sh

e o mesmo para o arquivo hive-default.xml.template
$ cp hive-default.xml.template hive-default.xml

segue os arquivos gerados após a copia
[hadoop@dataserver conf]$ ls -la
total 636
drwxrwxr-x.  2 hadoop hadoop   4096 jan 13 11:54 .
drwxrwxr-x. 10 hadoop hadoop    184 jan 13 11:38 ..
-rw-r--r--.  1 hadoop hadoop   1596 ago 22 18:44 beeline-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop 300482 jan 13 11:54 hive-default.xml
-rw-r--r--.  1 hadoop hadoop 300482 ago 22 19:01 hive-default.xml.template
-rw-r--r--.  1 hadoop hadoop   2365 jan 13 11:52 hive-env.sh
-rw-r--r--.  1 hadoop hadoop   2365 ago 22 18:44 hive-env.sh.template
-rw-r--r--.  1 hadoop hadoop   2274 ago 22 18:45 hive-exec-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   3086 ago 22 18:44 hive-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   2060 ago 22 18:44 ivysettings.xml
-rw-r--r--.  1 hadoop hadoop   3558 ago 22 18:44 llap-cli-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   7163 ago 22 18:44 llap-daemon-log4j2.properties.template
-rw-r--r--.  1 hadoop hadoop   2662 ago 22 18:44 parquet-logging.properties

Após gerar as devidas copias devemos verificar o conteudo dos arquivos com oo seguinte comando;
$ gedit hive-env.sh

Ajustar as seguintes linhas;
# Set HADOOP_HOME to point to a specific hadoop install directory
# HADOOP_HOME=${bin}/../../hadoop
HADOOP_HOME=/opt/hadoop

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/hive/conf

$ gedit .bashrc
# Hive
export HIVE_HOME=/opt/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:.

$ gedit hive-default.xml
<property>
    <name>hive.metastore.event.db.notification.api.auth</name>
    <value>false<value/>
    <description>
	O metastore deve autorizar contra APIs relacionadas à notificação do banco de dados, como get_next_notification.
	Se definido como true, somente os superusuários nas configurações de proxy terão permissão.
    <description/>
 <property>

Também será necessário validar outros diretórios, pois esses estão dentro do HDFS.
Acesse o diretório HDFS através do comando
$ cd ~
$ hdfs dfs -ls /user
Dentro do diretorio user deve ser criado um novo diretório "hive"
$ hdfs dfs -mkdir /user/hive

Dentro do diretorio /user/hive/warehouse também necessaŕio cria o diretório "warehouse"

segue sequencia:
[hadoop@dataserver ~]$ hdfs dfs -ls /
Found 2 items
drwx------   - hadoop supergroup          0 2019-11-30 23:59 /tmp
drwxr-xr-x   - hadoop supergroup          0 2019-11-26 22:51 /user
[hadoop@dataserver ~]$ hdfs dfs -ls /user
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2019-12-07 18:36 /user/hadoop
[hadoop@dataserver ~]$ hdfs dfs -mkdir /user/hive
[hadoop@dataserver ~]$ hdfs dfs -ls /user
Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2019-12-07 18:36 /user/hadoop
drwxr-xr-x   - hadoop supergroup          0 2020-01-13 20:31 /user/hive
[hadoop@dataserver ~]$ hdfs dfs -mkdir /user/hive/warehouse
[hadoop@dataserver ~]$ hdfs dfs -ls /user
Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2019-12-07 18:36 /user/hadoop
drwxr-xr-x   - hadoop supergroup          0 2020-01-13 20:34 /user/hive
[hadoop@dataserver ~]$ hdfs dfs -ls /user/hive
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2020-01-13 20:34 /user/hive/warehouse

Dentro do doretório Warehouse será armazenado todo o squema gerado referente as informações.

Importante identificar o seguinte erro:
Estava com esse mesmo problema mas resolvi da seguinte forma. Vai no caminho /opt/hadoop/share/hadoop/common/lib e verifica a versao do arquivo guava . jar . Provavelmente ele sera diferente da versao desse jar que esta presente no HIVE /opt/hive/lib .
Caso a versão esteja diferente , apague a versão mais antiga e copie a versao mais nova para ambos os diretorios , tanto o do hadoop quanto o do HIVE . Resumindo, a versão do guava .jar tatnto do hadoop quanto do hive tem que ser a mesma.

Inializando o squema hive
$ schematool -dbType derby -initSchema

Necessário gerar o schema, pois se trata de banco relacional por esse motivo é necessário gerar o schema.

Observação importante: O banco de dados usado será o "derby", pois poderia ser outro qualquer como SQL, ORACLE e etc..



SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 3.1.0
Initialization script hive-schema-3.1.0.derby.sql

Initialization script completed
schemaTool completed

Após esses procedimento execute o comando
$ hive
Será aberto o shell do hive

Para acessar a tabela
hive> show tables;

Para sair
hive> exit;



======================================================================

Instalando e configurando o Pig

O Pig fornece um mecanismo para executar fluxos de dados em paralelo no Hadoop. Ele inclui um idioma, Pig Latin, para expressar esses fluxos de dados. O Pig Latin inclui operadores para muitas das operações de dados tradicionais (junção, classificação, filtro etc.), bem como a capacidade dos usuários de desenvolver suas próprias funções para leitura, processamento e gravação de dados. Porco é executado no Hadoop. Ele utiliza o sistema de arquivos distribuídos Hadoop, HDFS e o sistema de processamento do Hadoop, MapReduce.
O Pig usa o MapReduce para executar todo o seu processamento de dados. Ele compila os scripts do Pig Latin que os usuários gravam em uma série de uma ou mais tarefas do MapReduce que são executadas. O Pig Latin parece diferente de muitas das linguagens de programação que você já viu. Não há instruções if ou para loops no Pig Latin. Isso ocorre porque as linguagens de programação tradicionais de procedimentos e orientadas a objetos descrevem o fluxo de controle, e o fluxo de dados é um efeito colateral do programa. O Pig Latin se concentra no fluxo de dados.

Apache Pig server para grandes volumes de dados.

Acessar a pagina 
https://pig.apache.org/
Link -  Apache Pig 0.17.0 is released!

News
Apache Pig 0.17.0 is released!

The highlights of this release is the introduction of Pig on Spark. See details on the 

link - release page.

link - Download
Download a release now!

acesse:
http://ftp.unicamp.br/pub/apache/pig

link - pig-0.17.0/ 
Ponto de observação, caso não tiver o bin baixar o tar.

Após fazer o Download do Pig descompactar.
$ tar -xvf pig-0.17.0.tar.gz

Mover o arquivo para o diretorio /opt/pig
$ sudo mv pig-0.17.0 /opt/pig

acessar o arquivo para ajustar os dados de ambientes
$ gedit .bashrc

# Pig
export PIG_HOME=/opt/pig
export PATH=$PATH:$PIG_HOME/bin
export PIG_CLASSPATH=$HADOOP_HOME/conf

Executa o seguinte comando para acessar o shell do PIG.
$ pig
$ pig help;

Para sair
$ quit:


======================================================================

Instalando e configurando Spark

Estrutura de computação de cluster de análise de dados originalmente desenvolvida no AMPLab da UC Berkeley. O Spark se encaixa na comunidade de código aberto do Hadoop, baseada no HDFS (Hadoop Distributed File System). No entanto, o Spark fornece uma alternativa mais fácil de usar ao Hadoop MapReduce e oferece desempenho até 10 vezes mais rápido que os sistemas da geração anterior, como o Hadoop MapReduce, para determinados aplicativos.
O Spark é uma estrutura para escrever programas distribuídos rapidamente. O Spark resolve problemas semelhantes aos do Hadoop MapReduce, mas com uma abordagem rápida na memória e uma API de estilo funcional limpa. Com sua capacidade de integração com o Hadoop e ferramentas embutidas para análise interativa de consultas (Shark), processamento e análise de gráficos em larga escala (Bagel) e análise em tempo real (Spark Streaming), ele pode ser usado de forma interativa para processar e consultar rapidamente grandes conjuntos de dados.
Para acelerar a programação, o Spark fornece APIs limpas e concisas em Scala, Java e Python. Você também pode usar o Spark interativamente dos shells Scala e Python para consultar rapidamente grandes conjuntos de dados. O Spark também é o mecanismo por trás do Shark, um sistema de data warehouse totalmente compatível com Apache Hive que pode executar 100x mais rápido que o Hive.

Acesse o Link
http://hadoopecosystemtable.github.io/
E consulte por Spark va até "Apache Spark"

Para Baixar acesse o link:
http://spark.apache.org/downloads.html
Verifique qual a versão do hadoop ou superior e clique na opção 3 para ser direcionado ao mirror
Pagina do mirror
https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz

Acesse o mirror de preferencia
HTTP¶
Faça o Download
http://ftp.unicamp.br/pub/apache/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz

Descompactar o arquivo
$ tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz

Apos descompactar, copie o arquivo para o diretorio padrão /opt/spark
$ sudo mv spark-3.0.0-preview2-bin-hadoop3.2 /opt/spark

Não será preciso fazer nenhuma configuração dentro da pasta Spark, portanto devemos voltar ao diretorio cd ~ 

Acesse o .bashrc para editar variaveis de ambientes:
$ gedit .bashrc

# Spark
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin

$ source .bashrc

Para executar o Spark :
spark-shell

Esse Shell é para atuar com Shell da linguagem Scala, caso queira usar a api python segue o comando para acesso

Primeiro sai do shell-scala:
scala> :q

Agora para acessar a api Python, digite o seguinte comando:
[hadoop@dataserver ~]$ pyspark

Caso o comando pyspark não funcione será necessário executar os seguintes procedimentos:

1 - Vá para a pasta $ cd /opt/spark, acesse a pasta conf

2 - Na pasta conf, existe um arquivo chamado spark-env.sh. Caso você tenha um arquivo chamado spark-env.sh.template, precisará copiar o arquivo para um novo arquivo chamado spark-env.sh.

3 - Edite o arquivo e escreva as próximas três linhas
export PYSPARK_PYTHON =/usr/bin/python3
export PYSPARK_DRIVER_PYTHON =/usr/bin/python3
export SPARK_YARN_USER_ENV = "PYSPARK_PYTHON =/usr/bin/python3"

Dessa forma, se você baixar uma nova versão independente do Spark, poderá definir a versão do Python para a qual deseja executar o PySpark

Caminho para executar o python localmente.
$ cd /usr/bin/ env python3


======================================================================

Instalando e configurando Apache Sqoop

Sistema para transferência de dados em massa entre HDFS e datastores estruturados como RDBMS. Como o Flume, mas do HDFS para o RDBMS. Portanto pode funcionar como uma ferramenta ETL.

Acesse a pagina:
http://sqoop.apache.org/

Existe uma ponto muito importante no Sqoop que dentro da documentação podemos identificar que são as versões disponiveis, pois termos que usar a opção de Sqoop 1, pois as versões de Sqoop 2 não é aconselhavel usar, essas se encontram incompletas e com muitos problemas.


Iremos usar o seguinte link:
Documentation >> Sqoop 1 >> Downloads
 
 HTTP

http://ftp.unicamp.br/pub/apache/sqoop/1.4.7

Baixar o link:
sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

Após baixar o arquivo será necessário descompactar com o seguinte comando:
$ tar -xvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

Com o arquivo descompactado será necessário mover o diretório para /opt/sqoop com o seguinte comando.

$ sudo mv sqoop-1.4.7.bin__hadoop-2.6.0 /opt/sqoop

Dentro do diretório devemos criar dois diretórios
$ cd /opt/sqoop

$ sudo mkdir accumulo
sudo mkdir hcatalog
 
Acesse a pasta conf/
Copie o arquivo 
$ sudo cp sqoop-env-template. sqoop-env.sh

Execute o comando :
gedit sqoop-env.sh

e ajuste as linhas de variaveis conforme abaixo:
# Set Hadoop-specific environment variables here.

#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/opt/hadoop

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/opt/hadoop

#set the path to where bin/hbase is available
export HBASE_HOME=/opt/hbase

#Set the path to where bin/hive is available
export HIVE_HOME=/opt/hive

#Set the path for where zookeper config dir is
export ZOOCFGDIR=/opt/zookeeper/conf


Apos editar também será necessário acessar o .bashrc
$ gedit .bashrc
Crie as seguintes variaveis:
# Sqoop
export SQOOP_HOME=/opt/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
export HCAT_HOME=/opt/sqoop/hcatalog
export ACCUMULO_HOME=/opt/sqoop/accumulo


Feito as configurações de variaveis execute o comando :
$ sqoop version
O importante está nessa linha , pois informa que esta rodando com a seguinte versão 1.4.7
2020-01-22 00:44:53,479 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

Toda essa configuração possibilita executa e exportar dados de um banco relacional para o HDFS através do SQOOP

=================================================================

Instalando e configurando o Apache Flume

O Flume é um serviço distribuído, confiável e disponível para coletar, agregar e mover com eficiência grandes quantidades de dados de log. Possui uma arquitetura simples e flexível, baseada no fluxo de dados de streaming. É robusto e tolerante a falhas, com mecanismos de confiabilidade ajustáveis ​​e muitos mecanismos de failover e recuperação. Ele usa um modelo de dados extensível simples que permite aplicativos analíticos online.

Acesse a pagina oficial
http://flume.apache.org/

Acesse o link Download
pache-flume-1.9.0-bin.tar.gz

HTTP

http://ftp.unicamp.br/pub/apache/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz

Após baixar o arquivo no diretório downloads descompactar
$ tar -xvf apache-flume-1.9.0-bin.tar.gz

Mover para o diretório /opt/flume
$ sudo mv apache-flume-1.9.0-bin /opt/flume

$ cd /opt/flume

Acesse o arquivo conf/
$ cd conf/

Execute o comando para copiar o arquivo
$ sudo cp flume-env.sh.template flume-env.sh

[hadoop@dataserver conf]$ sudo cp flume-env.sh.template flume-env.sh
[hadoop@dataserver conf]$ ls -la
total 20
drwxr-xr-x. 2 hadoop hadoop  147 jan 22 02:05 .
drwxrwxr-x. 7 hadoop hadoop  187 jan 22 02:00 ..
-rw-r--r--. 1 hadoop hadoop 1661 nov 16  2017 flume-conf.properties.template
-rw-r--r--. 1 hadoop hadoop 1455 nov 16  2017 flume-env.ps1.template
-rw-r--r--. 1 root   root   1568 jan 22 02:05 flume-env.sh
-rw-r--r--. 1 hadoop hadoop 1568 ago 30  2018 flume-env.sh.template
-rw-rw-r--. 1 hadoop hadoop 3107 dez 10  2018 log4j.properties

Recomendado sempre gerar uma copia conforme comando acima

Execute o comando dentro do arquivo flume-env.sh
gedit flume-env.sh

Será necessário ajustar as veriaveis conforme abaixo, desomentar e incluir a seguinte variavel.
# Enviroment variables can be set here.

export JAVA_HOME=/opt/jdk

Feito o passo acima acesse as variaveis de ambiente .bashrc no diretorio home
/home/hadoop

Execute
gedit .bashrc

Incluir as seguintes variaveis de ambientes dentro do arquivo .bashrc
# Flume
export FLUME_HOME=/opt/flume
export PATH=$PATH:$FLUME_HOME/bin

Execute o comando 
hadoop@dataserver ~]$ flume-ng help
Usage: /opt/flume/bin/flume-ng <command> [options]...

commands:
  help                      display this help text
  agent                     run a Flume agent
  avro-client               run an avro Flume client
  version                   show Flume version info

global options:
  --conf,-c <conf>          use configs in <conf> directory
  --classpath,-C <cp>       append to the classpath
  --dryrun,-d               do not actually start Flume, just print the command
  --plugins-path <dirs>     colon-separated list of plugins.d directories. See the
                            plugins.d section in the user guide for more details.
                            Default: $FLUME_HOME/plugins.d
  -Dproperty=value          sets a Java system property value
  -Xproperty=value          sets a Java -X option

agent options:
  --name,-n <name>          the name of this agent (required)
  --conf-file,-f <file>     specify a config file (required if -z missing)
  --zkConnString,-z <str>   specify the ZooKeeper connection to use (required if -f missing)
  --zkBasePath,-p <path>    specify the base path in ZooKeeper for agent configs
  --no-reload-conf          do not reload config file if changed
  --help,-h                 display help text

avro-client options:
  --rpcProps,-P <file>   RPC client properties file with server connection params
  --host,-H <host>       hostname to which events will be sent
  --port,-p <port>       port of the avro source
  --dirname <dir>        directory to stream to avro source
  --filename,-F <file>   text file to stream to avro source (default: std input)
  --headerFile,-R <file> File containing event headers as key/value pairs on each new line
  --help,-h              display help text

  Either --rpcProps or both --host and --port must be specified.

Note that if <conf> directory is specified, then it is always included first
in the classpath.


=============================================================


Para Baixar grandes volumes de dados podemos acessar o seguinte Site
https://grouplens.org/
Menu -- Dataset

Devemos Baixar o arquivo ml-25.zip no meu caso o arquivo disponivel era esse para efeto de teste.

Importante apontar que quando for mover um arquivo dentro do hdfs devemos usar o -put

=============================================================
Parametrizando o hdfs

Por padrão o hdfs usa o diretório /tmp/hadoop-hadoop/dfs, portanto não é procedimento deixar 
os arquivos do hdfs dentro do diretório TMP, pois é um diretŕio temporario onde de tempo em 
tempos esse são excluidos.

Portanto podemos usar o procedimento abaixo para executar as devidas configurações para 
parametrizar o hdfs.

# Cria os diretórios abaixo

mkdir /opt/hadoop/dfs
mkdir /opt/hadoop/dfs/data - Esse diretório irá guardar os dados do datanode
mkdir /opt/hadoop/dfs/namespace_logs --- Esse diretório irá guardar os metadados do namenods

Como estamos em um piseudocluster devemos saber o namespace_logs ficará no namenode e o  data 
será criado em todos os cluster conforme ecosistema hadoop.


# Editar o arquivo $HADOOP_HOME/etc/hadoop/hdfs-site.xml e adicionar as linhas:

	Esse parametro abaixo será definido somente no namenode, caso sendo um multicluster
    
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:/opt/hadoop/dfs/namespace_logs</value>
    </property>

    Esse abaixo deve ser gerado dentro de todos os cluster datanode
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:/opt/hadoop/dfs/data</value>
    </property>

    Lembrando que os parametros devem ficar dentro da tag <configuration></configuration>

Lembrando que estamos dentro de um pseudo cluster .... poratnto se tivemos outras maquina cada parametro deve estar dentro da sua devida maquian datanode e namenode, 

Caso seja necessário apagar os dados do arquivo tmp segue o comando:
$ sudo rm -rf * ----Esse comando irá linpara todos os ados dentro do arquivo tmp.

=============================================================

Verificando status do banco de dados relacionnal MYSQL SEVER

Iniciar o serviço:
$ sudo systemctl start mysqld
$ sudo  systemctl status mysqld
Iniciando o processo o serviço deve estar como actve(running)

Para acessar o shel devemos executar o seguinte comando:
mysql -u root -p
Password = Pingo@@@1 

Para sair do shell mysql execute;
mysql> exit

=============================================================
Conectando Mysql no Hadoop.

Para isso acontecer devemos baixar o conector JODB MYSQL ou para o banco de origem que pode ser qualquer um conforme mercado.

Necessário acessar o site do fabricante no nosso caso será do MYSQL.
https://dev.mysql.com/downloads/connector/j/

Ou pode pesquisar no google como 
jdbc connector download

Apos acessar o link definido, podemos verificar a cersão disponivel e escolher para qual sistema operacional devemos apontar;
Portanto devemos escolher a plataforma independnete (Platform Independent), pois iremos baixar um arquivo .zip e não um pacote se caso escolher uma opção de sistema operacional com Debian ou outro.


Platform Independent (Architecture Independent), ZIP Archive >> clique em download
 Não é necessário criar conta >> No thanks, just start my download.

Descompact o arquivo 
# unzip mysql-connector-java-8.0.19
OBS: o arquivo está no diretorio Downloads 

localize o arquivo após decompactado:
mysql-connector-java-8.0.19.jar 


Feito todo o processo devemos copiar o arquivo para pasta lib(Biblioteca) do SqOOP com o seguinte comando:


$ cp mysql-connector-java-8.0.19.jar /opt/sqoop//lib/

Essa pasta lib ira conctar o sqoop ao banco de dados mysql que criamos conforme seu JDBC MYSQL.

O apche SQOOP é uma ferramenta ETL essa ferramenta ira pegar os dados o banco SGBD relacional e importar para o HDFS.

usamos o seguinte comando para acessar o banco de dados:
sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root -P

sqoop list-databases --connect jdbc:mysql://localhost:3306/testedb --username root -password Pingo@@@1 --table empregados --m 1

Erro: Não foi possível localizar nem carregar a classe principal org.apache.hadoop.hbase.util.GetJavaProperty

Conforme informações da equipe Datascience, esse erro é devido a não atualização de uma biblioteca, pois o apache sqoop não suporta a versão do hadoop apache 3. ou superior.


Para solução do problema iremos acessar o git da datscience para executar o procedimento:
Acesse o Seguinte link
https://github.com/dsacademybr/Libs/blob/master/commons-lang-2.6-bin.zip

Copie o link no botão de Download e baixe o arquivo via terminal

com o seguinte comando;
Baixar no diretório Downloads
$  wget https://github.com/dsacademybr/Libs/blob/master/commons-lang-2.6-bin.zip

Caso o arquivo apresente erro ao descompactar baixe direto do botão Downloads

Após baixar descompactar o arquivo
unzip commons-lang-2.6-bin.zip

Devemos copiar o arquivos que fica dentro do diretorio commons-lang-2.6
commons-lang-2.6.jar para o diretorio /lib do sqoop com o seguinte comando:
$ cp commons-lang-2.6.jar /opt/sqoop/lib/


Erroo timezone
$ sqoop list-databases --connect jdbc:mysql://localhost:3306/?serverTimezone=UTC --username root -P

Comando para executar o import:
$ sqoop import --connect jdbc:mysql://localhost:3306/testedb?serverTimezone=UTC --username root --password Pingo@@@1 --table empregados --m 1 

Depois que o arquivo for gerado será necessário copiar para maquina local para identificar se o arquivo esta correto com o seguinte comando

$ hdfs dfs -get /user/hadoop/empregados/part-m-00000 /home/hadoop/Documentos/teste/

feito esse procedimento podemos verificar o arquivo com o comando:

$ cat Documentos/teste/part-m-00000 
Alessandro Faqueti,12345


======================================================================

Configurando anaconda

Consulte anaconda 
anaconda.com

Acesse Downloads e selcione o sistema operacional de referencia

BAixe a versão python3.7 version, pois a versão 2.7 não será usado no treinamento.

Após executar o Download executar o comando 
$ bash Anaconda3-2019.10-Linux-x86_64.sh
Dentro do Diretório Downloads/

Anaconda foiinstalado    no seguinte diretorio
/home/hadoop/anaconda3

Em caso de problemas o anaconda pode ser desintalados simplesmente deletar o diretorio anaconda.

Usado o pip para instalar o jupyter lab

$ sudo pip3 install jupyterlab

após instalar execute o comando:
$ jupyter lab

segue link de instalção 
https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html

Para configuração e uso do anaconda, precisamos fazer uma instalção de um pacote chamado "mrjob", pois esse pacote
contempla uma facilidade para gerar mapreduce no python conforme suas bibliotecas.

acesse o seguinte link:
https://github.com/Yelp/mrjob

e para instalar execute o seguinte comando:
$ pip install mrjob

==========================================================================

Executando MapReduce com Python

Primeiro vamos usar alguns exemplos:

Caso necessário dar permissão para usuário De root para usuário local segue o comando:
chown -R hadoop:hadoop *.* 
em caso de arquivos .py segue
chown -R hadoop:hadoop *.py

Dentro do diretório Datasets temos divesos datasets de exemplo, esse denvem ser copiados para o HDFS

$ hdfs dfs -put Dataset/OrgulhoePreconceito.txt /mapred

O arquivo é um livro baixado gratuitamente para usar da seguinte maneira:
Devemos identificar no resultado quantas vezes a mesma palavra aparece durante todo o conteudo do livro.

Feito o procedimento de copiar o arquivo para o HDFS devemos executar o seguinte comando:

Iremos usar o arquivo qe esta no diretório Analitics
[hadoop@dataserver Analytics]$ python MR-DataMining-3.py hdfs:///mapred/OrgulhoePreconceito.txt -r hadoop

Esse arquivo MR-DataMining-3.py irá executar todo o procedimento de MapReduce que precisamos.

Comando administrador para extrair relatorios do hdfs

$ dhfs dfsadmin -report
